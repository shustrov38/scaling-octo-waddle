version: '3.8'

services:
  spark-master:
    build:
      context: .
      dockerfile: docker/spark-master.Dockerfile
    image: lakehouse-spark-master:latest
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - spark-network
    volumes:
      - ./data:/app/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5

  spark-worker:
    build:
      context: .
      dockerfile: docker/spark-worker.Dockerfile
    image: lakehouse-spark-worker:latest
    container_name: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    networks:
      - spark-network
    volumes:
      - ./data:/app/data
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 5

  mlflow:
    image: python:3.9-slim
    container_name: mlflow
    ports:
      - "5000:5000"
    networks:
      - spark-network
    volumes:
      - ./mlruns:/app/mlruns
    command: >
      sh -c "pip install mlflow &&
              mlflow server --host 0.0.0.0 --port 5000 
              --backend-store-uri file:///app/mlruns 
              --default-artifact-root file:///app/mlruns"

  app:
    build:
      context: .
      dockerfile: docker/app.Dockerfile
    image: lakehouse-app:latest
    container_name: lakehouse-app
    depends_on:
      spark-master:
        condition: service_healthy
      spark-worker:
        condition: service_healthy
    networks:
      - spark-network
    volumes:
      - ./data:/app/data
      - ./mlruns:/app/mlruns
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://spark-master:8080"]
      interval: 30s
      timeout: 10s
      retries: 5

networks:
  spark-network:
    driver: bridge

volumes:
  data:
  mlruns: